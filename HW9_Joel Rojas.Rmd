---
title: "HW9 by Joel Rojas"
output: github_document
---
install.packages("knitr")


### Econ B2000, MA Econometrics
### HW9
### Fall 2023



  
We will start the first of a 3-part sequence.

Upon Claudia Goldin being named for Nobel Prize this month, I was talking with a colleague, Prof Norma Fuentes-Mayorga. She described her hypothesis about female labor force participation and especially choices to work in the public sector, in jobs that are more stable. She has heard this rationale in numerous interviews, particularly among minoritized women, but we'd like to see if there is more evidence for this. I have created an indicator (public_work) in the ACS2021 data, based on the industry that the person works in.

In the first part, we will use basic OLS to estimate some models of the choice to work in public sector. In second part (next week) we'll estimate with logit and probit models. In third part (week after) we'll use some additional machine-learning techniques.

We'll start by discussing what is appropriate specification of interaction terms, to find evidence of this effect. In your group you can discuss about what subset is most relevant and how exactly you'd implement the estimation. Then you will work on some results, come back and share.

You'll have to download a couple csv definition files, `IND_levels.csv` and `publicwork_recode.csv`. Then you should first run these:

```{r eval=FALSE}
require(plyr)
require(dplyr)
require(tidyverse)
require(haven)


levels_n <- read.csv("IND_levels.csv")
names(levels_n) <- c("New_Level","levels_orig")
acs2017$IND <- as.factor(acs2017$IND)
levels_orig <- levels(acs2017$IND) 
levels_new <- join(data.frame(levels_orig),data.frame(levels_n))

acs2017$public_work <- acs2017$IND 

levels_public <- read.csv("publicwork_recode.csv")
names(levels_public) <- c("levels_orig","New_Level")
levels_new_pub <- join(data.frame(levels_orig),data.frame(levels_public))


levels(acs2017$IND) <- levels_new$New_Level
levels(acs2017$public_work) <- levels_new_pub$New_Level
summary(acs2017)

```

Now before you run to estimate a model, in general it is a good idea to check summary stats before doing fancier models. For example look at the fractions by education, maybe do some statistics like you ~~did~~ should have done in exam.

R doesn't want a factor as dependent variable in lm() call, so we create a numeric version,
```{r eval=FALSE}
acs2017$public_work_num <- as.numeric(acs2017$public_work == "work for public, stable")
```
Although other functions will take a factor. That can be trouble so be careful! All the math underlying is just concerned with which of the x-variables make the y-variable more likely to be a higher number. In this case it's ok, I've set it up but in general you want to confirm which factor answer is one and which is zero.

For instance,

```{r}
table(acs2017$public_work,acs2017$public_work_num)
#shows that a one corresponds to 'yes the person does work for public and/or in a generally stable job'. But a different person could estimate a model where the dependent variable is 'yes the person works in private sector' and that would have different signs for the estimated coefficients! Either model could be sensible, as long as you're clear about which one the computer is estimating. Be paranoid and check.
```

You can estimate models something like this (once you figure out what subset of data you'll use)
```{r eval=TRUE}


library(dplyr)

dat_use <- acs2017 %>%
  mutate(
    educ_hs = ifelse(EDUCD >= 060 & EDUCD <= 064, 1, 0),  
    educ_somecoll = ifelse(EDUCD >= 065 & EDUCD <= 083, 1, 0),  
    educ_college = ifelse(EDUCD >= 090 & EDUCD <= 101, 1, 0),  
    educ_advdeg = ifelse(EDUCD >= 110, 1, 0),
    female = ifelse(SEX == 2, 1, 0) 
  ) %>%
  filter(!is.na(public_work) & !is.na(public_work_num)) %>%
  filter( AGE >= 25, AGE <= 40)


ols_out1 <- lm(public_work_num ~ educ_hs + educ_somecoll + educ_college + educ_advdeg + AGE + female , data = dat_use)
summary(dat_use)
summary(ols_out1)


```
```

---
title: "Lab7 addendum"
output: github_document
---


  
### Econ B2000, MA Econometrics
### Kevin R Foster, the Colin Powell School at the City College of New York, CUNY
### Fall 2023

OK I already effed up the ordering by splitting Part 3 into 2 sub-parts. Now I'm going to further break Part2 into a main part and an addendum. 

This is some complicated coding to make later coding less complicated. It's an investment.

We want just 2 simple things: to standarize our data (all X-variables to have values just in [0,1] interval) and to split the data into a training set (that we use to estimate the model) and a test set (that we use to evaluate how well the model performs on new data that it hasn't trained on).

But depending on your model, doing just those 2 simple things can take a bit of work. Best to do that in the privacy of your own home.

I'll show this for a simple set of X-variables. Your version will be more complicated. The ones that are already nice dummy variables are easy and this coding might seem overly elaborate for them. But bigger factors such as the PUMA get ugly fast.

I'm not necessarily saying you should use PUMA in your regression, only that if you want to use a big factor with many levels, this is a way to do it. Each PUMA number codes a 'neighborhood' -- although the size of that neighborhood is trying to enclose a roughly equal number of people. Dense areas in NYC get small geographic areas but upstate, where people are sparse, the PUMAs can be large geographic areas. FYI, 4-digit codes starting with 37 are Bronx, 38 Manhattan, 39 SI, 40 Brooklyn and 41 Queens. You can find the codes if you'd like. But here I'll just leave the code number.


```{r eval=TRUE}

# fix each variable you want in your regression
# this example is for small version, 
# public_work_num ~ female + educ_hs + educ_somecoll + educ_college + educ_advdeg + AGE + PUMA_factor

# I want to demonstrate how to work with more complicated factors so I will also include PUMA
dat_use$PUMA_factor <- as.factor(dat_use$PUMA)

d_pub_work <- data.frame(model.matrix(~ dat_use$public_work_num)) 

d_female <- data.frame(model.matrix(~ dat_use$female))
d_educ_hs <- data.frame(model.matrix(~ dat_use$educ_hs))
d_educ_somecoll <- data.frame(model.matrix(~ dat_use$educ_somecoll))
d_educ_college <- data.frame(model.matrix(~ dat_use$educ_college))
d_educ_advdeg <- data.frame(model.matrix(~ dat_use$educ_advdeg))
d_age <- data.frame(model.matrix(~ dat_use$AGE))
d_PUMA <- data.frame(model.matrix(~ dat_use$PUMA_factor)) # which is really big!

```
In this step (and later) I worry that I don't want to accidentally create factors that are empty. Depending on your subgroup that you choose, this might happen. That will cause problems for later estimation (the math tries to answer the question, how are the zero observations in some group different from the other groups?). So we want to catch the problem early. Run colSums() to verify.

```{r eval=TRUE}
sum( colSums(d_PUMA) == 0) # should be zero
```
Then this puts them all together,
```{r eval=TRUE}


# there are better ways to code this, but this should be more robust to your other choices

dat_for_analysis_sub <- data.frame(
  d_pub_work[,2], # need [] since model.matrix includes intercept term
  d_female[,2],
  d_educ_hs[,2],
  d_educ_somecoll[,2],
  d_educ_college[,2],
  d_educ_advdeg[,2],
  d_age[,2],
  d_PUMA[,2:145] ) # this last term is why model.matrix 


# this is just about me being anal-retentive, see difference in names(dat_for_analysis_sub) before and after running this bit
names(dat_for_analysis_sub)
names(dat_for_analysis_sub) <- sub("dat_use.","",names(dat_for_analysis_sub)) # drops each repetition of dat_use

names(dat_for_analysis_sub)[1] <- "pub_work"
names(dat_for_analysis_sub)[2] <- "female"
names(dat_for_analysis_sub)[3:6] <- c("HS","SomeColl","College","AdvDeg")
names(dat_for_analysis_sub)[7] <- "Age"

names(dat_for_analysis_sub)

```
Then to create training data and test data,
```{r}
require("standardize")
set.seed(654321)
NN <- length(dat_for_analysis_sub$pub_work)

restrict_1 <- (runif(NN) < 0.1) # use 10% as training data, ordinarily this would be much bigger but start small
summary(restrict_1)
dat_train <- subset(dat_for_analysis_sub, restrict_1)
dat_test <- subset(dat_for_analysis_sub, !restrict_1)

# again check this below, should be zero
sum( colSums(dat_train) == 0)
```



Now writing the formula is a bit of a pain. Would like to have 'pub_work ~ female + HS + SomeColl + COllege + AdvDeg + Age + PUMA' but that last term is no longer an easy factor but a mess of 144 dummies! Don't copy-paste 144 times, instead:

```{r eval=TRUE}
fmla_sobj <- reformulate( names(dat_for_analysis_sub[2:151]), response = "pub_work")

sobj <- standardize(fmla_sobj, dat_train, family = binomial)

print(sobj)
s_dat_test <- predict(sobj, dat_test)




```

Now your OLS and logit models can be run like this:

```{r eval=TRUE}

model_lpm1 <- lm(sobj$formula, data = sobj$data)
summary(model_lpm1)
pred_vals_lpm <- predict(model_lpm1, s_dat_test)
pred_model_lpm1 <- (pred_vals_lpm > mean(pred_vals_lpm))
table(pred = pred_model_lpm1, true = dat_test$pub_work)

# logit 
model_logit1 <- glm(sobj$formula, family = binomial, data = sobj$data)
summary(model_logit1)
pred_vals <- predict(model_logit1, s_dat_test, type = "response")
pred_model_logit1 <- (pred_vals > 0.5)
table(pred = pred_model_logit1, true = dat_test$pub_work)

```

---
title: "Lab 8"
output: github_document
---

### Econ B2000, MA Econometrics
### Kevin R Foster, the Colin Powell School at the City College of New York, CUNY
### Fall 2023

This is part 3 A of a 3-part segment. Part 3 B will be the fifth part of a 3-part segment, because econometricians count good.

In the last 2 weeks we estimated OLS and logit models. Last week's addendum included basics on how to create a standardized dataset split into training and test. We'll move to estimate other models in a variety of specifications.

Start with a review. Some of these machine learning techniques are very computationally intensive so we might want to split up the problems. You should convince yourselves that these estimations provide identical coefficient estimates (once properly interpreted):

```{r eval=FALSE}

# for now, really simplify the education dummy
dat_use$BA_plus <- dat_use$educ_college + dat_use$educ_advdeg

# whole dataset
model_lpm_v1 <- lm(public_work_num ~ female + BA_plus + AGE + I(female*BA_plus) + I(AGE * female), data = dat_use)
summary(model_lpm_v1)

dat_use_female <- subset(dat_use,as.logical(dat_use$female))
dat_use_male <- subset(dat_use,!(dat_use$female))

# now split into 2 parts
model_lpm_v1f <- lm(public_work_num ~ BA_plus + AGE, data = dat_use_female)
summary(model_lpm_v1f)
model_lpm_v1m <- lm(public_work_num ~ BA_plus + AGE, data = dat_use_male)
summary(model_lpm_v1m)


```

(Actually that's kinda nice exam question!) Please convince yourself (and explain) that the model predictions for different people are identical in the big equation with full interactions or the subsets. The big model with full interactions is tougher to interpret although it does provide easy access to hypothesis tests about the split (are the coefficients on age statistically different, for males or females?).

The point is that sometimes you might want to split the data into even smaller subsets, in order to run the model without crashing or taking hours. Your own computer has its limitations and you need to learn how to work around those. It's useful for future work -- many problems have vast amounts of data that need to be trimmed down. (There's a whole segment of data scientists who don't worry about their data size since they just splash it all onto AWS and then complain about how much they're paying AWS.)

A word of caution: just because a particular technique works better or worse on this particular dataset does *not* mean that it's always better or worse. Just in this particular case for these particular data.

One thing that we keep coming back to, in this class, is that there is both art and science to data analytics. The methods look so mathy and technical and have imposing names but there is quite a lot of personal artistry in how to use them. There is good and bad to the artistry (and also generally accepted vs unusual) but I want you to remember that fancy estimations don't guarantee correct results.

Sometimes it can be useful to try these techniques in sequence. Some are good at wringing out every drop of juice for prediction; others are good at selecting which variables don't give enough juice to be worth the squeeze. You might find some of your X-variables that are consistently not selected as useful for prediction.

In previous lab, you'd set a subsample and figured a set of X variables that are plausibly causal. You made choices about how to deal with NA values. You created this thing, `sobj <- standardize(y ~ X1 + X2 ...)` where you had choices for X variables and filled in the `...` part. You estimated OLS and logit and created confusion matrix for each, and checked predicted values overall and for subgroups.

Now let's estimate some fancier models.

Here is code for a Random Forest, which takes a bit of computing (especially if you include 144 PUMA dummies!),
```{r eval = FALSE}
require('randomForest')

set.seed(54321)
data_sample <- sobj$data[sample(nrow(sobj$data), 10000), ]
model_randFor <- randomForest(as.factor(pub_work) ~ ., data = data_sample, importance=TRUE, proximity=TRUE)


print(model_randFor)
round(importance(model_randFor),2)
varImpPlot(model_randFor)
# look at confusion matrix for this too
pred_model1 <- predict(model_randFor,  s_dat_test)
table(pred = pred_model1, true = dat_test$pub_work)
```
Note that the estimation prints out a Confusion Matrix first but that's within the training data; the later one calculates how well it does on the test data.

Next is Support Vector Machines. First it tries to find optimal tuning parameter, next uses those optimal values to train. (Tuning takes a long time so skip for now!)
```{r eval = FALSE}
require(e1071)
# tuned_parameters <- tune.svm(as.factor(pub_work) ~ ., data = sobj$data, gamma = 10^(-3:0), cost = 10^(-2:2)) 
# summary(tuned_parameters)
# figure best parameters and input into next
svm.model <- svm(as.factor(pub_work) ~ ., data = data_sample, cost = 1, gamma = 0.1)
test_sample <- s_dat_test[sample(nrow(s_dat_test), 10000), ]
svm.pred <- predict(svm.model, test_sample)
summary(svm.pred)
summary(test_sample)
table(pred = svm.pred, true = data_sample$pub_work)


```

When you summarize, you should be able to explain which models predict best (noting if there is a tradeoff of false positive vs false negative) and if there are certain explanatory variables that are consistently more or less useful. Also try other lists of explanatory variables.

Explain carefully about what is the marginal product of each of these methods. Old-fashioned OLS and logit give some predictions -- are these other methods better overall or in particular cases? Do they tend to make the same sort of errors or different ones? (If different then perhaps you can create an ensemble of models?)